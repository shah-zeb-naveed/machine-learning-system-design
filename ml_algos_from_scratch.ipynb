{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ9dsr7yjU4mJNMdO4+MF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shah-zeb-naveed/machine-learning-system-design/blob/main/ml_algos_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Implement cosine similarity between two vectors.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "  \"\"\"Calculates the cosine similarity between two vectors.\n",
        "\n",
        "  Args:\n",
        "    vec1: The first vector.\n",
        "    vec2: The second vector.\n",
        "\n",
        "  Returns:\n",
        "    The cosine similarity between the two vectors.\n",
        "  \"\"\"\n",
        "  dot_product = np.dot(vec1, vec2)\n",
        "  magnitude_vec1 = np.linalg.norm(vec1)\n",
        "  magnitude_vec2 = np.linalg.norm(vec2)\n",
        "  if magnitude_vec1 == 0 or magnitude_vec2 == 0:\n",
        "    return 0  # Handle cases where either vector has zero magnitude.\n",
        "  return dot_product / (magnitude_vec1 * magnitude_vec2)\n",
        "\n",
        "vec1 = [1, 5]\n",
        "vec2 = [3, 6]\n",
        "cosine_similarity(vec1, vec2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wxEDMX6v9Va",
        "outputId": "0f8a684f-4845-4c95-db46-fabc2fc194ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.9647638212377322)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean(vec1, vec2):\n",
        "  \"\"\"Calculates the Euclidean distance between two vectors.\n",
        "\n",
        "  Args:\n",
        "    vec1: The first vector.\n",
        "    vec2: The second vector.\n",
        "\n",
        "  Returns:\n",
        "    The Euclidean distance between the two vectors.\n",
        "  \"\"\"\n",
        "  # replace for loop with vectorization\n",
        "  return np.sqrt(np.sum((vec1 - vec2) ** 2))\n",
        "\n",
        "vec1 = np.array([1, 5])\n",
        "vec2 = np.array([3, 6])\n",
        "euclidean(vec1, vec2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKiZTMdDwYLS",
        "outputId": "3c320980-4d2e-452e-c243-da58abc738c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(2.23606797749979)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create function that calculates softmax\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Calculates the softmax of a vector.\n",
        "\n",
        "    Args:\n",
        "      x: A NumPy array or list representing the input vector.\n",
        "\n",
        "    Returns:\n",
        "      A NumPy array representing the softmax of the input vector.\n",
        "    \"\"\"\n",
        "    e_x = np.exp(x) # - np.max(x))  # Subtract max for numerical stability\n",
        "    return e_x / np.sum(np.exp(x))\n",
        "\n",
        "x = np.array([1, 2, 3])\n",
        "softmax(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53RuyaSuw8LJ",
        "outputId": "d642c5d9-00b1-4ed1-dd27-e5b8f5611c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.09003057, 0.24472847, 0.66524096])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create code snippet for a simple softmax classifier\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Example usage:\n",
        "scores = np.array([1.0, 2.0, 3.0])  # Example scores for three classes\n",
        "probabilities = np.argmax(softmax(scores))\n",
        "probabilities\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22qc_roCEcy4",
        "outputId": "cc9cff03-c1f3-48e8-babc-f7ac9a9779cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Gini impurity and entropy calculators.\n",
        "\n",
        "def gini_at_node(proportions):\n",
        "  \"\"\"Calculates the Gini impurity at a node given class proportions.\n",
        "\n",
        "  Args:\n",
        "    proportions: A list of proportions of each class at the node.\n",
        "                 The proportions should sum to 1.\n",
        "\n",
        "  Returns:\n",
        "    The Gini impurity at the node.\n",
        "  \"\"\"\n",
        "  gini = 1 - np.sum(np.square(proportions))\n",
        "  return gini\n",
        "\n",
        "def entropy_at_node(proportions):\n",
        "  \"\"\"Calculates the entropy at a node given class proportions.\n",
        "\n",
        "  Args:\n",
        "    proportions: A list of proportions of each class at the node.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  entropy = -np.sum(proportions * np.log2(proportions))\n",
        "  return entropy\n",
        "\n",
        "proportions = [0.5, 0.5]\n",
        "gini_at_node(proportions), entropy_at_node(proportions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvNCgQnyxp2S",
        "outputId": "b00c1371-f947-415c-f8a7-7e9e7825297e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(np.float64(0.5), np.float64(1.0))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# this is the gini of children. subtract from gini of parent to get gain.\n",
        "def gini_of_children(left_child, right_child):\n",
        "    n_left, n_right = len(left_child), len(right_child)\n",
        "    n_total = n_left + n_right\n",
        "\n",
        "    def gini(node):\n",
        "        if len(node) == 0:\n",
        "            return 0.0\n",
        "        _, counts = np.unique(node, return_counts=True)\n",
        "        probs = counts / counts.sum()\n",
        "        return 1.0 - np.sum(probs**2)\n",
        "\n",
        "    gini_left = gini(left_child)\n",
        "    gini_right = gini(right_child)\n",
        "\n",
        "    return (n_left / n_total) * gini_left + (n_right / n_total) * gini_right\n",
        "\n",
        "left_child = np.array([0, 0, 0, 0, 0, 1, 1])\n",
        "right_child = np.array([1, 1, 1, 1, 1, 0, 0])\n",
        "\n",
        "gini_impurity_value = gini_of_children(left_child, right_child)\n",
        "gini_impurity_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc36Z1mzzFLl",
        "outputId": "cfddcdd5-95e0-4eca-dda3-39e415673575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.40816326530612246)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate code for # KNN (including distance computation)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # lazy learner. nothing computed here.\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = [self._predict(x) for x in X]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # compute distances\n",
        "        distances = [euclidean(x, x_train) for x_train in self.X_train]\n",
        "\n",
        "        # get k nearest samples, labels\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
        "\n",
        "        # majority vote, most common class label\n",
        "        most_common = np.argmax(np.bincount(k_nearest_labels))\n",
        "\n",
        "        # if regression, return np.mean(k_nearest_labels)\n",
        "        return most_common\n",
        "\n",
        "example_X = np.array([[6, 11], [25, 24], [3, 4], [4, 5]])\n",
        "example_y = np.array([0, 0, 1, 1])\n",
        "\n",
        "knn = KNN(k=3)\n",
        "knn.fit(example_X, example_y)\n",
        "knn.predict(np.array([[2, 2]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU5IAlG03xtw",
        "outputId": "32c41509-a0ea-4844-b2b9-c7847290662c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Linear transformation\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        # Sigmoid activation (probabilities)\n",
        "        predictions = self.sigmoid(z)\n",
        "        return predictions\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "        self.weights = np.zeros(num_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            # Forward pass\n",
        "            predictions = self.forward(X)\n",
        "\n",
        "            # Compute gradients via chain rule\n",
        "            # log loss simplifies to\n",
        "            dw = (1 / num_samples) * np.dot(X.T, (predictions - y))\n",
        "            db = (1 / num_samples) * np.sum(predictions - y)\n",
        "\n",
        "            # Update weights\n",
        "            self.weights += self.learning_rate * (-dw)\n",
        "            self.bias += self.learning_rate * (-db)\n",
        "\n",
        "# Example input\n",
        "X_train = np.array([[1, 2], [2, 3], [3, 4]])  # 3 samples, 2 features\n",
        "y_train = np.array([0, 1, 0])  # Target labels\n",
        "\n",
        "# Create and fit the model\n",
        "model = LogisticRegression(learning_rate=0.01, epochs=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions for new data\n",
        "predictions = model.forward(np.array([[1, 1]]))  # Forward pass with new sample\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRPh7fH56U7v",
        "outputId": "f2b881c5-b7ae-443a-bb9f-096103cc4d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [0.42013584]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLFIn05LMMkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a function for cross-entropy loss\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    \"\"\"Calculates the cross-entropy loss between true and predicted labels.\n",
        "\n",
        "    Args:\n",
        "        y_true: A NumPy array of true labels (one-hot encoded).\n",
        "        y_pred: A NumPy array of predicted probabilities.\n",
        "\n",
        "    Returns:\n",
        "        The cross-entropy loss.\n",
        "    \"\"\"\n",
        "    epsilon = 1e-15  # Small value to avoid log(0) errors\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon) #numerical stability. for 0 and 1\n",
        "    loss = -np.sum(y_true * np.log(y_pred)) / len(y_true)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "nLs8hnB5Pi_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create func for MSE loss\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    \"\"\"Calculates the mean squared error (MSE) loss.\n",
        "\n",
        "    Args:\n",
        "        y_true: A NumPy array of true labels.\n",
        "        y_pred: A NumPy array of predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        The MSE loss.\n",
        "    \"\"\"\n",
        "    return np.mean((y_true - y_pred)**2)\n"
      ],
      "metadata": {
        "id": "KlkRWBBt0dx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Decision tree node splitting (Gini impurity or entropy)."
      ],
      "metadata": {
        "id": "EdEvVtp-tjEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: A function to find best split in DT\n",
        "\n",
        "import numpy as np\n",
        "def best_split(X, y):\n",
        "    \"\"\"Finds the best split for a decision tree node.\n",
        "\n",
        "    Args:\n",
        "        X: The feature matrix.\n",
        "        y: The target vector.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the best split feature index, the best split threshold,\n",
        "        and the minimum Gini impurity.\n",
        "    \"\"\"\n",
        "    best_feature_index = None\n",
        "    best_threshold = None\n",
        "    num_features = X.shape[1]\n",
        "    min_gini_impurity = 1.0  # Initialize with the maximum possible value\n",
        "\n",
        "    for feature_index in range(num_features):\n",
        "        thresholds = np.unique(X[:, feature_index])\n",
        "        for threshold in thresholds:\n",
        "            left_child_indices = X[:, feature_index] <= threshold\n",
        "            right_child_indices = X[:, feature_index] > threshold\n",
        "            left_child_labels = y[left_child_indices]\n",
        "            right_child_labels = y[right_child_indices]\n",
        "            gini_impurity_gain = gini_of_children(left_child_labels, right_child_labels)\n",
        "\n",
        "            if gini_impurity_gain < min_gini_impurity:\n",
        "                min_gini_impurity = gini_impurity_gain\n",
        "                best_feature_index = feature_index\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_feature_index, best_threshold, min_gini_impurity\n",
        "\n",
        "# example\n",
        "\n",
        "# data for current split\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
        "y = np.array([0, 0, 1, 1])\n",
        "\n",
        "best_feature_index, best_threshold, min_gini_impurity = best_split(X, y)\n",
        "print(best_feature_index, best_threshold, min_gini_impurity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QutwYBgCtdxk",
        "outputId": "f35544ed-5044-42cc-b31c-f7baeee80f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a code snippet for a very basic decision tree algorithm\n",
        "\n",
        "import numpy as np\n",
        "class Node:\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value # the leaf value. if not leaf, None.\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, min_samples_split=2, max_depth=2):\n",
        "        self.root = None\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def _build_tree(self, X, y, depth=0):\n",
        "        num_samples, num_features = X.shape\n",
        "        num_labels = len(np.unique(y))\n",
        "\n",
        "        # Splitting criteria\n",
        "        if depth >= self.max_depth or num_samples < self.min_samples_split or num_labels == 1:\n",
        "            leaf_value = self._most_common_label(y)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        best_feature_index, best_threshold, _ = best_split(X,y)\n",
        "\n",
        "        left_child_indices = X[:, best_feature_index] <= best_threshold\n",
        "        right_child_indices = X[:, best_feature_index] > best_threshold\n",
        "\n",
        "        left = self._build_tree(X[left_child_indices], y[left_child_indices], depth + 1)\n",
        "        right = self._build_tree(X[right_child_indices], y[right_child_indices], depth + 1)\n",
        "        return Node(best_feature_index, best_threshold, left, right)\n",
        "\n",
        "\n",
        "    def _most_common_label(self, y):\n",
        "        return np.argmax(np.bincount(y))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y)\n",
        "\n",
        "    def _predict_one(self, x, node):\n",
        "        if node.value is not None:\n",
        "            # this is a leaf\n",
        "            return node.value\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self._predict_one(x, node.left)\n",
        "        return self._predict_one(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict_one(x, self.root) for x in X]\n"
      ],
      "metadata": {
        "id": "g56XNrLIqPbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Linear Regression class with batch and mini-bastch gradient descent, with regularization option\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class LinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, n_iters=1000, regularization=None, lambda_param=0.1):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.regularization = regularization\n",
        "        self.lambda_param = lambda_param\n",
        "\n",
        "    def _run_batch_gradient_descent(self, X, y):\n",
        "      n_samples = X.shape[0]\n",
        "      y_predicted = self.predict(X)\n",
        "\n",
        "      # chain rule\n",
        "      dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "      db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "      # Regularization\n",
        "      if self.regularization == 'l1':\n",
        "        dw += (self.lambda_param / n_samples) * np.sign(self.weights)\n",
        "      elif self.regularization == 'l2':\n",
        "        dw += (2 * self.lambda_param / n_samples) * self.weights\n",
        "\n",
        "      self.weights -= self.learning_rate * dw\n",
        "      self.bias -= self.learning_rate * db\n",
        "\n",
        "    def fit(self, X, y, batch_size=None):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # init parameters\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # train model\n",
        "        for _ in range(self.n_iters):\n",
        "          if batch_size is None: # gradient descent\n",
        "            for _ in range(self.n_iters):\n",
        "                self._run_batch_gradient_descent(X, y)\n",
        "          else: # Mini-batch gradient descent\n",
        "            for i in range(0, n_samples, batch_size):\n",
        "                X_batch = X[i:i+batch_size]\n",
        "                y_batch = y[i:i+batch_size]\n",
        "                self._run_batch_gradient_descent(X_batch, y_batch)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_approximated = np.dot(X, self.weights) + self.bias\n",
        "        return y_approximated\n",
        "\n",
        "# example\n",
        "\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
        "y = np.array([1, 2, 3, 4])\n",
        "reg = LinearRegression(learning_rate=0.01, n_iters=1000, regularization='l1', lambda_param=0.1)\n",
        "reg.fit(X, y, batch_size=2)\n",
        "print(reg.predict(X))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrC3DPWZwiJC",
        "outputId": "7f234dba-f56d-42d7-ea6e-33b7fffefaac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.06721502 2.02743833 2.98766165 3.94788496]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: implement a simple KFold class\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class KFoldMy:\n",
        "    def __init__(self, n_splits=5):\n",
        "        self.n_splits = n_splits\n",
        "\n",
        "    def split(self, X, y=None):\n",
        "        n_samples = len(X)\n",
        "\n",
        "        # n_splits is really the number of possible test folds\n",
        "        # or the combination\n",
        "\n",
        "        # divide by n_samples to get an appropriate fold_size\n",
        "        # that will achieve n_splits\n",
        "\n",
        "        fold_size = n_samples // self.n_splits\n",
        "        indices = np.arange(n_samples)\n",
        "\n",
        "        print('fold_size', fold_size)\n",
        "        print('n_samples', n_samples)\n",
        "\n",
        "\n",
        "        for i in range(self.n_splits):\n",
        "            start = i * fold_size\n",
        "            end = start + fold_size\n",
        "            #end = start + fold_size\n",
        "            test_indices = indices[start:end]\n",
        "            train_indices = np.concatenate([indices[:start], indices[end:]])\n",
        "            print(len(train_indices), len(test_indices))\n",
        "            print(train_indices, test_indices)\n",
        "            yield train_indices, test_indices"
      ],
      "metadata": {
        "id": "9mjGbFZL4OkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: K-fold cross validation\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def k_fold_cross_validation(model, X, y, k=5):\n",
        "    kf = KFoldMy(n_splits=k) # , shuffle=True, random_state=42\n",
        "    scores = []\n",
        "    for train_index, test_index in kf.split(X):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        #model.fit(X_train, y_train)\n",
        "        predictions = y_test#model.predict(X_test)\n",
        "        score = mse_loss(y_test, predictions)\n",
        "        scores.append(score)\n",
        "\n",
        "    return np.mean(scores), np.std(scores)\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5,6],[6,7], [2, 3], [3, 4], [4, 5], [5,6],[6,7]])\n",
        "y = np.array([1, 2, 3, 4, 5, 6, 2, 3, 4, 5, 6])\n",
        "\n",
        "print('input:', len(X))\n",
        "reg = LinearRegression()\n",
        "mean_score, std_score = k_fold_cross_validation(reg, X, y, k=3)\n",
        "print(f\"Mean loss: {mean_score:.4f}, Standard deviation: {std_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVtO4CDyzWG8",
        "outputId": "a2f43a9a-2ec1-4448-8d6f-85f1b6978ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: 11\n",
            "fold_size 3\n",
            "n_samples 11\n",
            "8 3\n",
            "[ 3  4  5  6  7  8  9 10] [0 1 2]\n",
            "8 3\n",
            "[ 0  1  2  6  7  8  9 10] [3 4 5]\n",
            "8 3\n",
            "[ 0  1  2  3  4  5  9 10] [6 7 8]\n",
            "Mean loss: 0.0000, Standard deviation: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate func for stratified sampling\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def stratified_sampling(df, column, num_samples):\n",
        "    \"\"\"\n",
        "    Performs stratified sampling on a Pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "      df: The input DataFrame.\n",
        "      column: The column to stratify by.\n",
        "      num_samples: The total number of samples to draw.\n",
        "\n",
        "    Returns:\n",
        "      A new DataFrame with the stratified sample.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate proportions for each stratum\n",
        "    proportions = df[column].value_counts(normalize=True)\n",
        "\n",
        "    # Calculate the number of samples for each stratum\n",
        "    samples_per_stratum = (proportions * num_samples).astype(int)\n",
        "\n",
        "    # Adjust sample sizes to match total num_samples (due to rounding)\n",
        "    diff = num_samples - samples_per_stratum.sum()\n",
        "    if diff > 0 :\n",
        "        samples_per_stratum[samples_per_stratum.index[0]] += diff\n",
        "\n",
        "    # Sample from each stratum\n",
        "    stratified_sample = []\n",
        "    for stratum, num_samples_stratum in samples_per_stratum.items():\n",
        "      stratified_sample.append(df[df[column] == stratum].sample(n=num_samples_stratum, random_state=42))\n",
        "\n",
        "    # Concatenate the samples\n",
        "    return pd.concat(stratified_sample)\n"
      ],
      "metadata": {
        "id": "jZxBQmPJ_M1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create code snippet to implement simple kmeans from scratch\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class KMeans:\n",
        "  def __init__(self, n_clusters=8, max_iters=300):\n",
        "      self.n_clusters = n_clusters\n",
        "      self.max_iters = max_iters\n",
        "      self.centroids = None\n",
        "\n",
        "  def fit(self, X):\n",
        "      # Initialize centroids randomly. Initialize from dataset but eventually, no restriction.\n",
        "      # They can NOT belong to the dataset after training.\n",
        "      n_samples, n_features = X.shape\n",
        "      random_sample_idxs = np.random.choice(n_samples, self.n_clusters, replace=False)\n",
        "      self.centroids = X[random_sample_idxs]\n",
        "\n",
        "      for _ in range(self.max_iters):\n",
        "          # Assign each data point to the nearest centroid\n",
        "          clusters = [[] for _ in range(self.n_clusters)]\n",
        "          for data_point in X:\n",
        "              distances = [euclidean(data_point, centroid) for centroid in self.centroids]\n",
        "              closest_centroid_idx = np.argmin(distances)\n",
        "              clusters[closest_centroid_idx].append(data_point)\n",
        "\n",
        "          # Update centroids based on the mean of assigned data points\n",
        "          prev_centroids = self.centroids\n",
        "          self.centroids = []\n",
        "          for cluster in clusters:\n",
        "              if cluster: #check if the cluster is not empty\n",
        "                  new_centroid = np.mean(cluster, axis=0)\n",
        "                  self.centroids.append(new_centroid)\n",
        "              else:\n",
        "                  # If a cluster is empty, keep the old centroid OR even better, re-init to avoid dead centroid\n",
        "                  self.centroids.append(prev_centroids[len(self.centroids)])\n",
        "          self.centroids = np.array(self.centroids)  # Convert back to NumPy array\n",
        "\n",
        "          # Check for convergence\n",
        "          if np.all(prev_centroids == self.centroids):\n",
        "              break\n",
        "\n",
        "  def predict(self, X):\n",
        "      # Predict the cluster for each data point\n",
        "      predictions = []\n",
        "      for data_point in X:\n",
        "          distances = [euclidean(data_point, centroid) for centroid in self.centroids]\n",
        "          closest_centroid_idx = np.argmin(distances)\n",
        "          predictions.append(closest_centroid_idx)\n",
        "      return np.array(predictions)\n"
      ],
      "metadata": {
        "id": "si1Yhe1DBiXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: simulate classification data and calculate recall, precision, f1 score and ROC from scratch\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Simulate classification data\n",
        "np.random.seed(0)\n",
        "n_samples = 100\n",
        "X = np.random.rand(n_samples, 2)\n",
        "y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Example decision boundary\n",
        "\n",
        "# Example predictions (replace with your model's predictions)\n",
        "y_pred = (X[:, 0] + X[:, 1] > 0.8).astype(int)  # Another decision boundary\n",
        "y_prob = X[:, 0] + X[:, 1]"
      ],
      "metadata": {
        "id": "KakC7ewIFTI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create code snippet that calculates recall, precision, f1 and roc from scratch (do not use libraries like sklearn)\n",
        "\n",
        "import numpy as np\n",
        "def calculate_metrics(y_true, y_pred, y_prob):\n",
        "    \"\"\"Calculates recall, precision, F1-score, and ROC AUC from scratch.\n",
        "\n",
        "    Args:\n",
        "        y_true: True binary labels (0 or 1).\n",
        "        y_pred: Predicted binary labels (0 or 1).\n",
        "        y_prob: Predicted probabilities for the positive class.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing recall, precision, F1-score, and ROC AUC.\n",
        "    \"\"\"\n",
        "\n",
        "    tp = sum((y_true == 1) & (y_pred == 1))\n",
        "    tn = sum((y_true == 0) & (y_pred == 0))\n",
        "    fp = sum((y_true == 0) & (y_pred == 1))\n",
        "    fn = sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    # ROC AUC calculation\n",
        "    sorted_indices = np.argsort(y_prob)[::-1]  # Sort probabilities in descending order\n",
        "    y_true_sorted = y_true[sorted_indices]\n",
        "    y_prob_sorted = y_prob[sorted_indices]\n",
        "\n",
        "    tpr_values = []\n",
        "    fpr_values = []\n",
        "\n",
        "    # 0.9\n",
        "    # 0.5\n",
        "    # 0.1\n",
        "\n",
        "    for i in range(len(y_true_sorted)):\n",
        "      threshold = y_prob_sorted[i]\n",
        "      y_pred_threshold = (y_prob >= threshold).astype(int)\n",
        "\n",
        "      tp_t = sum((y_true == 1) & (y_pred_threshold == 1))\n",
        "      tn_t = sum((y_true == 0) & (y_pred_threshold == 0))\n",
        "      fp_t = sum((y_true == 0) & (y_pred_threshold == 1))\n",
        "      fn_t = sum((y_true == 1) & (y_pred_threshold == 0))\n",
        "\n",
        "      tpr = tp_t / (tp_t + fn_t) if (tp_t + fn_t) > 0 else 0\n",
        "      fpr = fp_t / (fp_t + tn_t) if (fp_t + tn_t) > 0 else 0\n",
        "\n",
        "      tpr_values.append(tpr)\n",
        "      fpr_values.append(fpr)\n",
        "\n",
        "    # trapezoidal rule\n",
        "    # just base multiplied by avg. height\n",
        "    auc = 0\n",
        "    for i in range(len(fpr_values) - 1):\n",
        "\n",
        "        auc += (fpr_values[i+1] - fpr_values[i]) * (tpr_values[i+1] + tpr_values[i]) / 2\n",
        "\n",
        "    return {\"recall\": recall, \"precision\": precision, \"f1\": f1, \"roc_auc\": auc}\n",
        "\n",
        "# Example usage (replace with your actual data):\n",
        "metrics = calculate_metrics(y, y_pred, y_prob)\n",
        "metrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8D1YZc3Febb",
        "outputId": "f79e5c32-c5e0-461b-9dc9-ad5f8ecd60b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'recall': np.float64(1.0),\n",
              " 'precision': np.float64(0.7647058823529411),\n",
              " 'f1': np.float64(0.8666666666666666),\n",
              " 'roc_auc': np.float64(1.0)}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate metrics\n",
        "recall = recall_score(y, y_pred)\n",
        "precision = precision_score(y, y_pred)\n",
        "f1 = f1_score(y, y_pred)\n",
        "roc_auc = roc_auc_score(y, y_prob)  # Use probabilities for ROC AUC\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y, y_prob)\n",
        "\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"F1-score: {f1:.2f}\")\n",
        "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Npp4eAYDFG5b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
